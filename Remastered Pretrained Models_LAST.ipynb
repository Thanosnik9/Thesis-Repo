{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f971ac9-6b1c-4037-a11f-c62eb628ebc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Training data shape: (7248, 64, 64, 3)\n",
      "Validation data shape: (2416, 64, 64, 3)\n",
      "Test data shape: (2416, 64, 64, 3)\n",
      "Training labels shape: (7248, 2)\n",
      "Validation labels shape: (2416, 2)\n",
      "Test labels shape: (2416, 2)\n",
      "Class weights: {0: 0.8511038046031001, 1: 1.2120401337792641}\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 10s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94668760/94668760 [==============================] - 12s 0us/step\n",
      "Training MobileNetV2 model...\n",
      "Epoch 1/200\n",
      "226/226 [==============================] - 24s 68ms/step - loss: 0.9279 - accuracy: 0.4877 - val_loss: 0.8010 - val_accuracy: 0.4913 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.8647 - accuracy: 0.5033 - val_loss: 0.7518 - val_accuracy: 0.5128 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 15s 66ms/step - loss: 0.8384 - accuracy: 0.4992 - val_loss: 0.7459 - val_accuracy: 0.5116 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 14s 64ms/step - loss: 0.8028 - accuracy: 0.5151 - val_loss: 0.7212 - val_accuracy: 0.5306 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.7903 - accuracy: 0.5107 - val_loss: 0.7371 - val_accuracy: 0.5116 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.7739 - accuracy: 0.5187 - val_loss: 0.7214 - val_accuracy: 0.5356 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7671 - accuracy: 0.5127 - val_loss: 0.7143 - val_accuracy: 0.5294 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7481 - accuracy: 0.5205 - val_loss: 0.6923 - val_accuracy: 0.5555 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.7415 - accuracy: 0.5237 - val_loss: 0.6944 - val_accuracy: 0.5666 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7351 - accuracy: 0.5256 - val_loss: 0.6971 - val_accuracy: 0.5513 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 15s 65ms/step - loss: 0.7311 - accuracy: 0.5216 - val_loss: 0.7033 - val_accuracy: 0.5414 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7218 - accuracy: 0.5366 - val_loss: 0.6961 - val_accuracy: 0.5546 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7234 - accuracy: 0.5291 - val_loss: 0.6873 - val_accuracy: 0.5509 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7184 - accuracy: 0.5356 - val_loss: 0.6986 - val_accuracy: 0.5435 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7120 - accuracy: 0.5377 - val_loss: 0.6837 - val_accuracy: 0.5671 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.7103 - accuracy: 0.5377 - val_loss: 0.6935 - val_accuracy: 0.5435 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7074 - accuracy: 0.5326 - val_loss: 0.6936 - val_accuracy: 0.5472 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7094 - accuracy: 0.5320 - val_loss: 0.6968 - val_accuracy: 0.5277 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6997 - accuracy: 0.5425 - val_loss: 0.6860 - val_accuracy: 0.5666 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "225/226 [============================>.] - ETA: 0s - loss: 0.7017 - accuracy: 0.5369\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7023 - accuracy: 0.5363 - val_loss: 0.6878 - val_accuracy: 0.5604 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 14s 64ms/step - loss: 0.6981 - accuracy: 0.5471 - val_loss: 0.6935 - val_accuracy: 0.5493 - lr: 2.0000e-05\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6971 - accuracy: 0.5442 - val_loss: 0.6906 - val_accuracy: 0.5538 - lr: 2.0000e-05\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 15s 68ms/step - loss: 0.6994 - accuracy: 0.5366 - val_loss: 0.6848 - val_accuracy: 0.5600 - lr: 2.0000e-05\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 15s 64ms/step - loss: 0.6977 - accuracy: 0.5392 - val_loss: 0.6927 - val_accuracy: 0.5538 - lr: 2.0000e-05\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6993 - accuracy: 0.5414 - val_loss: 0.6772 - val_accuracy: 0.5844 - lr: 2.0000e-05\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6992 - accuracy: 0.5333 - val_loss: 0.6883 - val_accuracy: 0.5476 - lr: 2.0000e-05\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.6971 - accuracy: 0.5484 - val_loss: 0.6795 - val_accuracy: 0.5745 - lr: 2.0000e-05\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.7008 - accuracy: 0.5377 - val_loss: 0.6821 - val_accuracy: 0.5774 - lr: 2.0000e-05\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.6973 - accuracy: 0.5435 - val_loss: 0.6837 - val_accuracy: 0.5646 - lr: 2.0000e-05\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.5448\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6953 - accuracy: 0.5448 - val_loss: 0.6828 - val_accuracy: 0.5695 - lr: 2.0000e-05\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6974 - accuracy: 0.5405 - val_loss: 0.6907 - val_accuracy: 0.5381 - lr: 4.0000e-06\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.6963 - accuracy: 0.5455 - val_loss: 0.6785 - val_accuracy: 0.5762 - lr: 4.0000e-06\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 14s 62ms/step - loss: 0.6977 - accuracy: 0.5438 - val_loss: 0.6853 - val_accuracy: 0.5666 - lr: 4.0000e-06\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6914 - accuracy: 0.5455 - val_loss: 0.6914 - val_accuracy: 0.5584 - lr: 4.0000e-06\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.5460\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "226/226 [==============================] - 14s 63ms/step - loss: 0.6973 - accuracy: 0.5460 - val_loss: 0.6813 - val_accuracy: 0.5737 - lr: 4.0000e-06\n",
      "Best MobileNetV2 fine-tuned model saved to best_MobileNetV2_model.h5\n",
      "76/76 [==============================] - 3s 27ms/step\n",
      "Training EfficientNetB0 model...\n",
      "Epoch 1/200\n",
      "226/226 [==============================] - 23s 75ms/step - loss: 0.7073 - accuracy: 0.4989 - val_loss: 0.6851 - val_accuracy: 0.5877 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 15s 66ms/step - loss: 0.7007 - accuracy: 0.5139 - val_loss: 0.6806 - val_accuracy: 0.5877 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.7078 - accuracy: 0.4866 - val_loss: 0.6989 - val_accuracy: 0.4123 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.7035 - accuracy: 0.5007 - val_loss: 0.6930 - val_accuracy: 0.5310 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.7044 - accuracy: 0.4939 - val_loss: 0.6933 - val_accuracy: 0.4636 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 15s 66ms/step - loss: 0.7001 - accuracy: 0.5097 - val_loss: 0.6906 - val_accuracy: 0.5877 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "225/226 [============================>.] - ETA: 0s - loss: 0.7048 - accuracy: 0.4954\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.7048 - accuracy: 0.4951 - val_loss: 0.7007 - val_accuracy: 0.4123 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 15s 66ms/step - loss: 0.7031 - accuracy: 0.4942 - val_loss: 0.6991 - val_accuracy: 0.4123 - lr: 2.0000e-05\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.7018 - accuracy: 0.4988 - val_loss: 0.6933 - val_accuracy: 0.4706 - lr: 2.0000e-05\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 15s 64ms/step - loss: 0.7019 - accuracy: 0.4986 - val_loss: 0.6965 - val_accuracy: 0.4123 - lr: 2.0000e-05\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.7012 - accuracy: 0.4986 - val_loss: 0.6914 - val_accuracy: 0.5877 - lr: 2.0000e-05\n",
      "Epoch 12/200\n",
      "225/226 [============================>.] - ETA: 0s - loss: 0.6974 - accuracy: 0.5099\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "226/226 [==============================] - 15s 67ms/step - loss: 0.6977 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5877 - lr: 2.0000e-05\n",
      "Best EfficientNetB0 fine-tuned model saved to best_EfficientNetB0_model.h5\n",
      "76/76 [==============================] - 5s 41ms/step\n",
      "Training ResNet50V2 model...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 24s 85ms/step - loss: 1.5100 - accuracy: 0.4972 - val_loss: 0.9342 - val_accuracy: 0.5178 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 1.1696 - accuracy: 0.5115 - val_loss: 0.8159 - val_accuracy: 0.5327 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 1.0508 - accuracy: 0.5140 - val_loss: 0.7622 - val_accuracy: 0.5563 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 17s 74ms/step - loss: 0.9993 - accuracy: 0.5051 - val_loss: 0.7737 - val_accuracy: 0.5373 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.9257 - accuracy: 0.5184 - val_loss: 0.7530 - val_accuracy: 0.5344 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.8650 - accuracy: 0.5223 - val_loss: 0.7335 - val_accuracy: 0.5559 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.8560 - accuracy: 0.5129 - val_loss: 0.7124 - val_accuracy: 0.5550 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.8123 - accuracy: 0.5292 - val_loss: 0.7473 - val_accuracy: 0.5344 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.7942 - accuracy: 0.5272 - val_loss: 0.7111 - val_accuracy: 0.5563 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.7750 - accuracy: 0.5281 - val_loss: 0.7041 - val_accuracy: 0.5497 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.7673 - accuracy: 0.5202 - val_loss: 0.6878 - val_accuracy: 0.5724 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7431 - accuracy: 0.5345 - val_loss: 0.7332 - val_accuracy: 0.5021 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7484 - accuracy: 0.5255 - val_loss: 0.7027 - val_accuracy: 0.5368 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 17s 75ms/step - loss: 0.7328 - accuracy: 0.5367 - val_loss: 0.6895 - val_accuracy: 0.5650 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7333 - accuracy: 0.5249 - val_loss: 0.6875 - val_accuracy: 0.5712 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7196 - accuracy: 0.5313 - val_loss: 0.6906 - val_accuracy: 0.5584 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 17s 75ms/step - loss: 0.7206 - accuracy: 0.5312 - val_loss: 0.7005 - val_accuracy: 0.5455 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7188 - accuracy: 0.5398 - val_loss: 0.6941 - val_accuracy: 0.5464 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.7099 - accuracy: 0.5356 - val_loss: 0.6838 - val_accuracy: 0.5828 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7104 - accuracy: 0.5414 - val_loss: 0.7116 - val_accuracy: 0.5099 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7074 - accuracy: 0.5400 - val_loss: 0.7164 - val_accuracy: 0.5157 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 17s 75ms/step - loss: 0.7107 - accuracy: 0.5360 - val_loss: 0.7017 - val_accuracy: 0.5381 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 18s 77ms/step - loss: 0.7087 - accuracy: 0.5330 - val_loss: 0.6886 - val_accuracy: 0.5501 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.7016 - accuracy: 0.5345\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "226/226 [==============================] - 18s 78ms/step - loss: 0.7016 - accuracy: 0.5345 - val_loss: 0.6922 - val_accuracy: 0.5476 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 18s 77ms/step - loss: 0.6969 - accuracy: 0.5358 - val_loss: 0.6773 - val_accuracy: 0.5712 - lr: 2.0000e-05\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.7033 - accuracy: 0.5396 - val_loss: 0.6752 - val_accuracy: 0.5902 - lr: 2.0000e-05\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6953 - accuracy: 0.5498 - val_loss: 0.6885 - val_accuracy: 0.5555 - lr: 2.0000e-05\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 18s 78ms/step - loss: 0.6974 - accuracy: 0.5484 - val_loss: 0.6939 - val_accuracy: 0.5368 - lr: 2.0000e-05\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.7006 - accuracy: 0.5461 - val_loss: 0.6745 - val_accuracy: 0.5873 - lr: 2.0000e-05\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6910 - accuracy: 0.5534 - val_loss: 0.6891 - val_accuracy: 0.5464 - lr: 2.0000e-05\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.6941 - accuracy: 0.5574 - val_loss: 0.6794 - val_accuracy: 0.5762 - lr: 2.0000e-05\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 17s 77ms/step - loss: 0.6923 - accuracy: 0.5491 - val_loss: 0.6796 - val_accuracy: 0.5737 - lr: 2.0000e-05\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6966 - accuracy: 0.5400 - val_loss: 0.6878 - val_accuracy: 0.5604 - lr: 2.0000e-05\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5493\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6933 - accuracy: 0.5493 - val_loss: 0.6833 - val_accuracy: 0.5579 - lr: 2.0000e-05\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6913 - accuracy: 0.5563 - val_loss: 0.6839 - val_accuracy: 0.5563 - lr: 4.0000e-06\n",
      "Epoch 36/200\n",
      "226/226 [==============================] - 17s 75ms/step - loss: 0.6936 - accuracy: 0.5531 - val_loss: 0.6839 - val_accuracy: 0.5588 - lr: 4.0000e-06\n",
      "Epoch 37/200\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6972 - accuracy: 0.5442 - val_loss: 0.6835 - val_accuracy: 0.5584 - lr: 4.0000e-06\n",
      "Epoch 38/200\n",
      "226/226 [==============================] - 17s 75ms/step - loss: 0.6924 - accuracy: 0.5419 - val_loss: 0.6772 - val_accuracy: 0.5728 - lr: 4.0000e-06\n",
      "Epoch 39/200\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.5556\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "226/226 [==============================] - 17s 76ms/step - loss: 0.6895 - accuracy: 0.5556 - val_loss: 0.6774 - val_accuracy: 0.5799 - lr: 4.0000e-06\n",
      "Best ResNet50V2 fine-tuned model saved to best_ResNet50V2_model.h5\n",
      "76/76 [==============================] - 5s 53ms/step\n",
      "Confusion Matrix (Test) for MobileNetV2:\n",
      "[[506 856]\n",
      " [279 775]]\n",
      "Classification Report (Test) for MobileNetV2:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.64      0.37      0.47      1362\n",
      "Damaged_augmented       0.48      0.74      0.58      1054\n",
      "\n",
      "         accuracy                           0.53      2416\n",
      "        macro avg       0.56      0.55      0.52      2416\n",
      "     weighted avg       0.57      0.53      0.52      2416\n",
      "\n",
      "Confusion Matrix (Test) for EfficientNetB0:\n",
      "[[1362    0]\n",
      " [1054    0]]\n",
      "Classification Report (Test) for EfficientNetB0:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.56      1.00      0.72      1362\n",
      "Damaged_augmented       0.00      0.00      0.00      1054\n",
      "\n",
      "         accuracy                           0.56      2416\n",
      "        macro avg       0.28      0.50      0.36      2416\n",
      "     weighted avg       0.32      0.56      0.41      2416\n",
      "\n",
      "Confusion Matrix (Test) for ResNet50V2:\n",
      "[[805 557]\n",
      " [427 627]]\n",
      "Classification Report (Test) for ResNet50V2:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.65      0.59      0.62      1362\n",
      "Damaged_augmented       0.53      0.59      0.56      1054\n",
      "\n",
      "         accuracy                           0.59      2416\n",
      "        macro avg       0.59      0.59      0.59      2416\n",
      "     weighted avg       0.60      0.59      0.59      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, ResNet50V2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\Θάνος\\\\Desktop\\\\Thesis Thanasis\\\\data_aug_3'\n",
    "subfolders = ['clear', 'clouds']\n",
    "categories = ['Healthy_augmented', 'Damaged_augmented']\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_data(base_dir, subfolders, categories, img_height, img_width):\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, subfolder, category)\n",
    "            images = sorted(os.listdir(folder_path))\n",
    "            for img_name in images:\n",
    "                if img_name.endswith('.png'):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                    data.append(img_array)\n",
    "                    labels.append(class_num)\n",
    "                    image_paths.append((subfolder, category, img_name))\n",
    "    return np.array(data), np.array(labels), image_paths\n",
    "\n",
    "data, labels, image_paths = load_data(base_dir, subfolders, categories, IMG_HEIGHT, IMG_WIDTH)\n",
    "data = data / 255.0\n",
    "\n",
    "# Split data ensuring twins are in the same split\n",
    "def split_data(image_paths):\n",
    "    unique_image_ids = list(set([img_name for subfolder, category, img_name in image_paths]))\n",
    "    train_ids, test_ids = train_test_split(unique_image_ids, test_size=0.2, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "def get_split_indices(image_paths, split_ids):\n",
    "    split_indices = [i for i, (subfolder, category, img_name) in enumerate(image_paths) if img_name in split_ids]\n",
    "    return split_indices\n",
    "\n",
    "train_ids, val_ids, test_ids = split_data(image_paths)\n",
    "train_indices = get_split_indices(image_paths, train_ids)\n",
    "val_indices = get_split_indices(image_paths, val_ids)\n",
    "test_indices = get_split_indices(image_paths, test_ids)\n",
    "\n",
    "X_train, y_train = data[train_indices], labels[train_indices]\n",
    "X_val, y_val = data[val_indices], labels[val_indices]\n",
    "X_test, y_test = data[test_indices], labels[test_indices]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Validation labels shape: {y_val.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Define data augmentation with seed\n",
    "def create_datagen(seed=None):\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ), seed\n",
    "\n",
    "datagen, seed = create_datagen(seed=42)  # Set the seed for reproducibility\n",
    "datagen.fit(X_train)\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, seed=seed)  # Use the seed here too\n",
    "val_generator = datagen.flow(X_val, y_val, batch_size=BATCH_SIZE, seed=seed)\n",
    "\n",
    "# Compute class weights using the training set\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Define model building functions\n",
    "def build_mobilenetv2_model(input_shape):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_efficientnetb0_model(input_shape):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_resnet50v2_model(input_shape):\n",
    "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "models = {\n",
    "    \"MobileNetV2\": build_mobilenetv2_model(input_shape),\n",
    "    \"EfficientNetB0\": build_efficientnetb0_model(input_shape),\n",
    "    \"ResNet50V2\": build_resnet50v2_model(input_shape)\n",
    "}\n",
    "\n",
    "# Compile models\n",
    "for name, model in models.items():\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train and evaluate each model\n",
    "histories = {}\n",
    "test_results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=200,\n",
    "        validation_data=val_generator,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    histories[name] = history\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model_path = f'best_{name}_model.h5'\n",
    "    #model.save(model_path)\n",
    "    print(f\"Best {name} fine-tuned model saved to {model_path}\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "    y_test_true = np.argmax(y_test, axis=1)\n",
    "    y_test_pred = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "    test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "    test_class_report = classification_report(y_test_true, y_test_pred, target_names=categories)\n",
    "\n",
    "    test_results[name] = {\n",
    "        \"confusion_matrix\": test_conf_matrix,\n",
    "        \"classification_report\": test_class_report\n",
    "    }\n",
    "\n",
    "# Print the evaluation results for each model\n",
    "for name, results in test_results.items():\n",
    "    print(f\"Confusion Matrix (Test) for {name}:\")\n",
    "    print(results[\"confusion_matrix\"])\n",
    "    print(f\"Classification Report (Test) for {name}:\")\n",
    "    print(results[\"classification_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752a3fc-d798-41a8-ac9f-5940fc782444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
