{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76454dd-8557-4e9f-b934-6c3919ffc912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Training data shape: (9664, 224, 224, 3)\n",
      "Test data shape: (2416, 224, 224, 3)\n",
      "Training labels shape: (9664, 2)\n",
      "Test labels shape: (2416, 2)\n",
      "Class weights: {0: 0.8516038068382094, 1: 1.2110275689223058}\n",
      "Epoch 1/50\n",
      "302/302 [==============================] - 80s 217ms/step - loss: 0.6881 - accuracy: 0.5714 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.6460 - accuracy: 0.6279 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 77s 253ms/step - loss: 0.6288 - accuracy: 0.6447 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 78s 257ms/step - loss: 0.6007 - accuracy: 0.6735 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 74s 244ms/step - loss: 0.5775 - accuracy: 0.6907 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.5390 - accuracy: 0.7201 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 73s 240ms/step - loss: 0.5095 - accuracy: 0.7470 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 74s 244ms/step - loss: 0.4661 - accuracy: 0.7757 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.4296 - accuracy: 0.7952 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.3783 - accuracy: 0.8265 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 77s 256ms/step - loss: 0.3316 - accuracy: 0.8512 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 75s 249ms/step - loss: 0.2712 - accuracy: 0.8855 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.2545 - accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 77s 254ms/step - loss: 0.2515 - accuracy: 0.8961 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 73s 241ms/step - loss: 0.2395 - accuracy: 0.9047 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.2233 - accuracy: 0.9116 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.2349 - accuracy: 0.9037 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 73s 241ms/step - loss: 0.2115 - accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.2085 - accuracy: 0.9134 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 73s 243ms/step - loss: 0.2059 - accuracy: 0.9162 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.2027 - accuracy: 0.9193 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 74s 243ms/step - loss: 0.2009 - accuracy: 0.9192 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.1896 - accuracy: 0.9218 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.1891 - accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.1830 - accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "302/302 [==============================] - 73s 243ms/step - loss: 0.1790 - accuracy: 0.9313 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "302/302 [==============================] - 76s 253ms/step - loss: 0.1736 - accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "302/302 [==============================] - 72s 237ms/step - loss: 0.1686 - accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "302/302 [==============================] - 74s 243ms/step - loss: 0.1699 - accuracy: 0.9319 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1650 - accuracy: 0.9335 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1616 - accuracy: 0.9364 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.1545 - accuracy: 0.9386 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1485 - accuracy: 0.9435 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1529 - accuracy: 0.9402 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.1422 - accuracy: 0.9418 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1417 - accuracy: 0.9444 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1414 - accuracy: 0.9464 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "302/302 [==============================] - 74s 243ms/step - loss: 0.1357 - accuracy: 0.9431 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1337 - accuracy: 0.9472 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "302/302 [==============================] - 74s 244ms/step - loss: 0.1283 - accuracy: 0.9499 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.1279 - accuracy: 0.9489 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "302/302 [==============================] - 73s 241ms/step - loss: 0.1301 - accuracy: 0.9488 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "302/302 [==============================] - 74s 244ms/step - loss: 0.1341 - accuracy: 0.9482 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "302/302 [==============================] - 73s 241ms/step - loss: 0.1241 - accuracy: 0.9498 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1260 - accuracy: 0.9530 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1226 - accuracy: 0.9530 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1176 - accuracy: 0.9552 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "302/302 [==============================] - 74s 244ms/step - loss: 0.1081 - accuracy: 0.9605 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1072 - accuracy: 0.9609 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.1103 - accuracy: 0.9572 - lr: 1.0000e-05\n",
      "EfficientNet fine-tuned model saved to efficientnet_fine_tuned.h5\n",
      "76/76 [==============================] - 8s 57ms/step\n",
      "Confusion Matrix (Test) for EfficientNet:\n",
      "[[889 477]\n",
      " [510 540]]\n",
      "Classification Report (Test) for EfficientNet:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.64      0.65      0.64      1366\n",
      "Damaged_augmented       0.53      0.51      0.52      1050\n",
      "\n",
      "         accuracy                           0.59      2416\n",
      "        macro avg       0.58      0.58      0.58      2416\n",
      "     weighted avg       0.59      0.59      0.59      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Update base_dir to the path where you uploaded your data\n",
    "base_dir = 'C:\\\\Users\\\\Θάνος\\\\Desktop\\\\Thesis Thanasis\\\\data_aug_3'\n",
    "subfolders = ['clear', 'clouds']\n",
    "categories = ['Healthy_augmented', 'Damaged_augmented']\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_data(base_dir, subfolders, categories, img_height, img_width):\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, subfolder, category)\n",
    "            images = sorted(os.listdir(folder_path))\n",
    "            for img_name in images:\n",
    "                if img_name.endswith('.png'):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                    data.append(img_array)\n",
    "                    labels.append(class_num)\n",
    "                    image_paths.append((subfolder, category, img_name))\n",
    "    return np.array(data), np.array(labels), image_paths\n",
    "\n",
    "data, labels, image_paths = load_data(base_dir, subfolders, categories, IMG_HEIGHT, IMG_WIDTH)\n",
    "data = data / 255.0\n",
    "\n",
    "# Split data ensuring twins are in the same split\n",
    "def split_data(image_paths):\n",
    "    unique_image_ids = list(set([img_name for subfolder, category, img_name in image_paths]))\n",
    "    train_ids, test_ids = train_test_split(unique_image_ids, test_size=0.2, random_state=42)\n",
    "    return train_ids, test_ids\n",
    "\n",
    "def get_split_indices(image_paths, split_ids):\n",
    "    split_indices = [i for i, (subfolder, category, img_name) in enumerate(image_paths) if img_name in split_ids]\n",
    "    return split_indices\n",
    "\n",
    "train_ids, test_ids = split_data(image_paths)\n",
    "train_indices = get_split_indices(image_paths, train_ids)\n",
    "test_indices = get_split_indices(image_paths, test_ids)\n",
    "\n",
    "X_train, y_train = data[train_indices], labels[train_indices]\n",
    "X_test, y_test = data[test_indices], labels[test_indices]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Define data augmentation with seed\n",
    "def create_datagen(seed=None):\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ), seed\n",
    "\n",
    "datagen, seed = create_datagen(seed=42)  # Set the seed for reproducibility\n",
    "datagen.fit(X_train)\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, seed=seed)  # Use the seed here too\n",
    "\n",
    "# Compute class weights using the training set\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Fine-tune pretrained models\n",
    "def fine_tune_pretrained_model(base_model, input_shape, num_classes=2):\n",
    "    base_model.trainable = True  # Allow fine-tuning\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Using EfficientNetB0 for better performance\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "model = fine_tune_pretrained_model(base_model, (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-4\n",
    "    if epoch > 10:\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Callbacks for training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=50,\n",
    "        callbacks=[reduce_lr, early_stopping, lr_scheduler],\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Save the fine-tuned model\n",
    "fine_tuned_model_path = 'efficientnet_fine_tuned.h5'\n",
    "#model.save(fine_tuned_model_path)\n",
    "print(f\"EfficientNet fine-tuned model saved to {fine_tuned_model_path}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions and true labels to label indices\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "y_test_pred = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Generate the confusion matrix for the test set\n",
    "test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for EfficientNet:\")\n",
    "print(test_conf_matrix)\n",
    "\n",
    "# Generate the classification report for the test set\n",
    "test_class_report = classification_report(y_test_true, y_test_pred, target_names=categories)\n",
    "\n",
    "print(f\"Classification Report (Test) for EfficientNet:\")\n",
    "print(test_class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093a159-5b93-4358-903c-10c891fd403b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc87132-6f0a-4cf4-bd6f-300f15befa44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b2ad05-4ba3-40f9-ac4f-4e1bcb7e2a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Training data shape: (9664, 224, 224, 3)\n",
      "Test data shape: (2416, 224, 224, 3)\n",
      "Training labels shape: (9664, 2)\n",
      "Test labels shape: (2416, 2)\n",
      "Class weights: {0: 0.8567375886524823, 1: 1.2007952286282306}\n",
      "Epoch 1/50\n",
      "302/302 [==============================] - 96s 262ms/step - loss: 0.6838 - accuracy: 0.5815 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 82s 269ms/step - loss: 0.6495 - accuracy: 0.6252 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 86s 284ms/step - loss: 0.6233 - accuracy: 0.6447 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 89s 294ms/step - loss: 0.6050 - accuracy: 0.6657 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 87s 288ms/step - loss: 0.5770 - accuracy: 0.6940 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 91s 300ms/step - loss: 0.5464 - accuracy: 0.7170 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 93s 308ms/step - loss: 0.5135 - accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 95s 313ms/step - loss: 0.4712 - accuracy: 0.7686 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 92s 304ms/step - loss: 0.4348 - accuracy: 0.7884 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 95s 315ms/step - loss: 0.3862 - accuracy: 0.8235 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 90s 297ms/step - loss: 0.3377 - accuracy: 0.8492 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 97s 321ms/step - loss: 0.2803 - accuracy: 0.8831 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 93s 308ms/step - loss: 0.2660 - accuracy: 0.8877 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 105s 346ms/step - loss: 0.2456 - accuracy: 0.9022 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 85s 282ms/step - loss: 0.2365 - accuracy: 0.9015 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 80s 265ms/step - loss: 0.2411 - accuracy: 0.8983 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 79s 262ms/step - loss: 0.2315 - accuracy: 0.9011 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 76s 251ms/step - loss: 0.2227 - accuracy: 0.9108 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 77s 253ms/step - loss: 0.2171 - accuracy: 0.9143 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 79s 260ms/step - loss: 0.2156 - accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 78s 257ms/step - loss: 0.2128 - accuracy: 0.9147 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 78s 256ms/step - loss: 0.1990 - accuracy: 0.9207 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.2001 - accuracy: 0.9199 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 75s 246ms/step - loss: 0.1915 - accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 76s 250ms/step - loss: 0.1950 - accuracy: 0.9215 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "302/302 [==============================] - 74s 243ms/step - loss: 0.1863 - accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.1833 - accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.1732 - accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.1761 - accuracy: 0.9283 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "302/302 [==============================] - 75s 249ms/step - loss: 0.1717 - accuracy: 0.9333 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1573 - accuracy: 0.9394 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1565 - accuracy: 0.9379 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "302/302 [==============================] - 78s 258ms/step - loss: 0.1593 - accuracy: 0.9380 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1561 - accuracy: 0.9408 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "302/302 [==============================] - 76s 250ms/step - loss: 0.1516 - accuracy: 0.9408 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.1407 - accuracy: 0.9441 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "302/302 [==============================] - 74s 243ms/step - loss: 0.1465 - accuracy: 0.9412 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "302/302 [==============================] - 74s 245ms/step - loss: 0.1495 - accuracy: 0.9435 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "302/302 [==============================] - 73s 241ms/step - loss: 0.1376 - accuracy: 0.9453 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "302/302 [==============================] - 80s 263ms/step - loss: 0.1307 - accuracy: 0.9505 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "302/302 [==============================] - 76s 252ms/step - loss: 0.1368 - accuracy: 0.9464 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "302/302 [==============================] - 79s 260ms/step - loss: 0.1300 - accuracy: 0.9488 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "302/302 [==============================] - 80s 263ms/step - loss: 0.1362 - accuracy: 0.9447 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1313 - accuracy: 0.9486 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "302/302 [==============================] - 75s 249ms/step - loss: 0.1237 - accuracy: 0.9521 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.1234 - accuracy: 0.9525 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "302/302 [==============================] - 73s 242ms/step - loss: 0.1132 - accuracy: 0.9569 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.1185 - accuracy: 0.9530 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "302/302 [==============================] - 75s 247ms/step - loss: 0.1135 - accuracy: 0.9573 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "302/302 [==============================] - 74s 246ms/step - loss: 0.1210 - accuracy: 0.9559 - lr: 1.0000e-05\n",
      "Epoch 1/50\n",
      "302/302 [==============================] - 86s 258ms/step - loss: 0.7207 - accuracy: 0.5785 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 81s 267ms/step - loss: 0.6399 - accuracy: 0.6399 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 80s 264ms/step - loss: 0.6178 - accuracy: 0.6515 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 77s 255ms/step - loss: 0.5948 - accuracy: 0.6777 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 76s 252ms/step - loss: 0.5720 - accuracy: 0.6987 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 76s 250ms/step - loss: 0.5452 - accuracy: 0.7228 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 76s 252ms/step - loss: 0.5177 - accuracy: 0.7387 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 75s 246ms/step - loss: 0.4733 - accuracy: 0.7727 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 74s 244ms/step - loss: 0.4400 - accuracy: 0.7942 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 79s 260ms/step - loss: 0.3968 - accuracy: 0.8227 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 79s 260ms/step - loss: 0.3754 - accuracy: 0.8311 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 79s 261ms/step - loss: 0.2429 - accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 77s 256ms/step - loss: 0.1913 - accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.1691 - accuracy: 0.9345 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 77s 255ms/step - loss: 0.1463 - accuracy: 0.9418 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 75s 248ms/step - loss: 0.1307 - accuracy: 0.9485 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 76s 250ms/step - loss: 0.1240 - accuracy: 0.9499 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 79s 260ms/step - loss: 0.1127 - accuracy: 0.9605 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 77s 253ms/step - loss: 0.0929 - accuracy: 0.9669 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 77s 253ms/step - loss: 0.0962 - accuracy: 0.9651 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 77s 256ms/step - loss: 0.0828 - accuracy: 0.9685 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 79s 259ms/step - loss: 0.0782 - accuracy: 0.9710 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 79s 260ms/step - loss: 0.0820 - accuracy: 0.9699 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 77s 254ms/step - loss: 0.0720 - accuracy: 0.9720 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 79s 262ms/step - loss: 0.0671 - accuracy: 0.9739 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "302/302 [==============================] - 80s 262ms/step - loss: 0.0606 - accuracy: 0.9780 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "302/302 [==============================] - 78s 257ms/step - loss: 0.0646 - accuracy: 0.9753 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "302/302 [==============================] - 80s 265ms/step - loss: 0.0594 - accuracy: 0.9791 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "302/302 [==============================] - 79s 262ms/step - loss: 0.0597 - accuracy: 0.9791 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "302/302 [==============================] - 82s 270ms/step - loss: 0.0471 - accuracy: 0.9817 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "302/302 [==============================] - 80s 264ms/step - loss: 0.0459 - accuracy: 0.9835 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "302/302 [==============================] - 78s 258ms/step - loss: 0.0443 - accuracy: 0.9855 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "302/302 [==============================] - 78s 257ms/step - loss: 0.0542 - accuracy: 0.9816 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "302/302 [==============================] - 76s 250ms/step - loss: 0.0405 - accuracy: 0.9860 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "302/302 [==============================] - 83s 276ms/step - loss: 0.0462 - accuracy: 0.9845 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "302/302 [==============================] - 87s 288ms/step - loss: 0.0416 - accuracy: 0.9851 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "302/302 [==============================] - 81s 267ms/step - loss: 0.0384 - accuracy: 0.9860 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "302/302 [==============================] - 83s 274ms/step - loss: 0.0383 - accuracy: 0.9870 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "302/302 [==============================] - 80s 265ms/step - loss: 0.0396 - accuracy: 0.9859 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "302/302 [==============================] - 83s 276ms/step - loss: 0.0338 - accuracy: 0.9888 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "302/302 [==============================] - 84s 277ms/step - loss: 0.0370 - accuracy: 0.9872 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "302/302 [==============================] - 85s 280ms/step - loss: 0.0319 - accuracy: 0.9892 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "302/302 [==============================] - 83s 274ms/step - loss: 0.0359 - accuracy: 0.9880 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "302/302 [==============================] - 83s 275ms/step - loss: 0.0325 - accuracy: 0.9891 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "302/302 [==============================] - 80s 263ms/step - loss: 0.0295 - accuracy: 0.9891 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "302/302 [==============================] - 81s 268ms/step - loss: 0.0331 - accuracy: 0.9882 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "302/302 [==============================] - 80s 264ms/step - loss: 0.0301 - accuracy: 0.9897 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "302/302 [==============================] - 83s 274ms/step - loss: 0.0281 - accuracy: 0.9902 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "302/302 [==============================] - 81s 267ms/step - loss: 0.0314 - accuracy: 0.9888 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "302/302 [==============================] - 80s 263ms/step - loss: 0.0279 - accuracy: 0.9892 - lr: 1.0000e-05\n",
      "Epoch 1/50\n",
      "302/302 [==============================] - 134s 382ms/step - loss: 0.6948 - accuracy: 0.4970 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 115s 379ms/step - loss: 0.6933 - accuracy: 0.5195 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 113s 372ms/step - loss: 0.6932 - accuracy: 0.5463 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 113s 374ms/step - loss: 0.6932 - accuracy: 0.4391 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 115s 379ms/step - loss: 0.6932 - accuracy: 0.4547 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 114s 378ms/step - loss: 0.6932 - accuracy: 0.5465 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 114s 377ms/step - loss: 0.6932 - accuracy: 0.4947 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4623\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "302/302 [==============================] - 114s 377ms/step - loss: 0.6932 - accuracy: 0.4623 - lr: 2.0000e-05\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 114s 377ms/step - loss: 0.6932 - accuracy: 0.4535 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4782 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.5644 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6931 - accuracy: 0.4797 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4930\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4930 - lr: 2.0000e-06\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4898 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6931 - accuracy: 0.4910 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4872 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 114s 377ms/step - loss: 0.6932 - accuracy: 0.4813 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4870\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4870 - lr: 2.0000e-06\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4829 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4859 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4927 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4848 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.4958\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6931 - accuracy: 0.4958 - lr: 2.0000e-06\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4894 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4846Restoring model weights from the end of the best epoch: 15.\n",
      "302/302 [==============================] - 114s 376ms/step - loss: 0.6932 - accuracy: 0.4846 - lr: 1.0000e-05\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m model_resnet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet_fine_tuned.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m model_vgg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg_fine_tuned.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 169\u001b[0m \u001b[43mmodel_efficientnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_efficientnet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m model_resnet\u001b[38;5;241m.\u001b[39msave(model_resnet_path)\n\u001b[0;32m    171\u001b[0m model_vgg\u001b[38;5;241m.\u001b[39msave(model_vgg_path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50, VGG16\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Update base_dir to the path where you uploaded your data\n",
    "base_dir = 'C:\\\\Users\\\\Θάνος\\\\Desktop\\\\Thesis Thanasis\\\\data_aug_3'\n",
    "subfolders = ['clear', 'clouds']\n",
    "categories = ['Healthy_augmented', 'Damaged_augmented']\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_data(base_dir, subfolders, categories, img_height, img_width):\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, subfolder, category)\n",
    "            images = sorted(os.listdir(folder_path))\n",
    "            for img_name in images:\n",
    "                if img_name.endswith('.png'):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                    data.append(img_array)\n",
    "                    labels.append(class_num)\n",
    "                    image_paths.append((subfolder, category, img_name))\n",
    "    return np.array(data), np.array(labels), image_paths\n",
    "\n",
    "data, labels, image_paths = load_data(base_dir, subfolders, categories, IMG_HEIGHT, IMG_WIDTH)\n",
    "data = data / 255.0\n",
    "\n",
    "# Split data ensuring twins are in the same split\n",
    "def split_data(image_paths):\n",
    "    unique_image_ids = list(set([img_name for subfolder, category, img_name in image_paths]))\n",
    "    train_ids, test_ids = train_test_split(unique_image_ids, test_size=0.2, random_state=42)\n",
    "    return train_ids, test_ids\n",
    "\n",
    "def get_split_indices(image_paths, split_ids):\n",
    "    split_indices = [i for i, (subfolder, category, img_name) in enumerate(image_paths) if img_name in split_ids]\n",
    "    return split_indices\n",
    "\n",
    "train_ids, test_ids = split_data(image_paths)\n",
    "train_indices = get_split_indices(image_paths, train_ids)\n",
    "test_indices = get_split_indices(image_paths, test_ids)\n",
    "\n",
    "X_train, y_train = data[train_indices], labels[train_indices]\n",
    "X_test, y_test = data[test_indices], labels[test_indices]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Define data augmentation with seed\n",
    "def create_datagen(seed=None):\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ), seed\n",
    "\n",
    "datagen, seed = create_datagen(seed=42)  # Set the seed for reproducibility\n",
    "datagen.fit(X_train)\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, seed=seed)  # Use the seed here too\n",
    "\n",
    "# Compute class weights using the training set\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Fine-tune pretrained models\n",
    "def fine_tune_pretrained_model(base_model, input_shape, num_classes=2):\n",
    "    base_model.trainable = True  # Allow fine-tuning\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Using EfficientNetB0 for better performance\n",
    "base_model_efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "model_efficientnet = fine_tune_pretrained_model(base_model_efficientnet, (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Using ResNet50 for better performance\n",
    "base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "model_resnet = fine_tune_pretrained_model(base_model_resnet, (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Using VGG16 for better performance\n",
    "base_model_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "model_vgg = fine_tune_pretrained_model(base_model_vgg, (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-4\n",
    "    if epoch > 10:\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Callbacks for training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the models\n",
    "with tf.device('/GPU:0'):\n",
    "    history_efficientnet = model_efficientnet.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=50,\n",
    "        callbacks=[reduce_lr, early_stopping, lr_scheduler],\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history_resnet = model_resnet.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=50,\n",
    "        callbacks=[reduce_lr, early_stopping, lr_scheduler],\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history_vgg = model_vgg.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=50,\n",
    "        callbacks=[reduce_lr, early_stopping, lr_scheduler],\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Save the fine-tuned models\n",
    "model_efficientnet_path = 'efficientnet_fine_tuned.h5'\n",
    "model_resnet_path = 'resnet_fine_tuned.h5'\n",
    "model_vgg_path = 'vgg_fine_tuned.h5'\n",
    "\n",
    "model_efficientnet.save(model_efficientnet_path)\n",
    "model_resnet.save(model_resnet_path)\n",
    "model_vgg.save(model_vgg_path)\n",
    "\n",
    "print(f\"EfficientNet fine-tuned model saved to {model_efficientnet_path}\")\n",
    "print(f\"ResNet fine-tuned model saved to {model_resnet_path}\")\n",
    "print(f\"VGG fine-tuned model saved to {model_vgg_path}\")\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "test_predictions_efficientnet = model_efficientnet.predict(X_test)\n",
    "test_predictions_resnet = model_resnet.predict(X_test)\n",
    "test_predictions_vgg = model_vgg.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions and true labels to label indices\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "y_test_pred_efficientnet = np.argmax(test_predictions_efficientnet, axis=1)\n",
    "y_test_pred_resnet = np.argmax(test_predictions_resnet, axis=1)\n",
    "y_test_pred_vgg = np.argmax(test_predictions_vgg, axis=1)\n",
    "\n",
    "# Generate the confusion matrix for the test set\n",
    "test_conf_matrix_efficientnet = confusion_matrix(y_test_true, y_test_pred_efficientnet)\n",
    "test_conf_matrix_resnet = confusion_matrix(y_test_true, y_test_pred_resnet)\n",
    "test_conf_matrix_vgg = confusion_matrix(y_test_true, y_test_pred_vgg)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for EfficientNet:\")\n",
    "print(test_conf_matrix_efficientnet)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for ResNet:\")\n",
    "print(test_conf_matrix_resnet)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for VGG:\")\n",
    "print(test_conf_matrix_vgg)\n",
    "\n",
    "# Generate the classification report for the test set\n",
    "test_class_report_efficientnet = classification_report(y_test_true, y_test_pred_efficientnet, target_names=categories)\n",
    "test_class_report_resnet = classification_report(y_test_true, y_test_pred_resnet, target_names=categories)\n",
    "test_class_report_vgg = classification_report(y_test_true, y_test_pred_vgg, target_names=categories)\n",
    "\n",
    "print(f\"Classification Report (Test) for EfficientNet:\")\n",
    "print(test_class_report_efficientnet)\n",
    "\n",
    "print(f\"Classification Report (Test) for ResNet:\")\n",
    "print(test_class_report_resnet)\n",
    "\n",
    "print(f\"Classification Report (Test) for VGG:\")\n",
    "print(test_class_report_vgg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377aebe0-c132-4422-9409-32ed88b339e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 75s 980ms/step\n",
      "76/76 [==============================] - 112s 1s/step\n",
      "76/76 [==============================] - 144s 2s/step\n",
      "Confusion Matrix (Test) for EfficientNet:\n",
      "[[909 491]\n",
      " [534 482]]\n",
      "Confusion Matrix (Test) for ResNet:\n",
      "[[937 463]\n",
      " [342 674]]\n",
      "Confusion Matrix (Test) for VGG:\n",
      "[[   0 1400]\n",
      " [   0 1016]]\n",
      "Classification Report (Test) for EfficientNet:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.63      0.65      0.64      1400\n",
      "Damaged_augmented       0.50      0.47      0.48      1016\n",
      "\n",
      "         accuracy                           0.58      2416\n",
      "        macro avg       0.56      0.56      0.56      2416\n",
      "     weighted avg       0.57      0.58      0.57      2416\n",
      "\n",
      "Classification Report (Test) for ResNet:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.73      0.67      0.70      1400\n",
      "Damaged_augmented       0.59      0.66      0.63      1016\n",
      "\n",
      "         accuracy                           0.67      2416\n",
      "        macro avg       0.66      0.67      0.66      2416\n",
      "     weighted avg       0.67      0.67      0.67      2416\n",
      "\n",
      "Classification Report (Test) for VGG:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.00      0.00      0.00      1400\n",
      "Damaged_augmented       0.42      1.00      0.59      1016\n",
      "\n",
      "         accuracy                           0.42      2416\n",
      "        macro avg       0.21      0.50      0.30      2416\n",
      "     weighted avg       0.18      0.42      0.25      2416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NickZografos\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\NickZografos\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\NickZografos\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models on the test set\n",
    "with tf.device('/CPU:0'):\n",
    "    test_predictions_efficientnet = model_efficientnet.predict(X_test)\n",
    "    test_predictions_resnet = model_resnet.predict(X_test)\n",
    "    test_predictions_vgg = model_vgg.predict(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions and true labels to label indices\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "y_test_pred_efficientnet = np.argmax(test_predictions_efficientnet, axis=1)\n",
    "y_test_pred_resnet = np.argmax(test_predictions_resnet, axis=1)\n",
    "y_test_pred_vgg = np.argmax(test_predictions_vgg, axis=1)\n",
    "\n",
    "# Generate the confusion matrix for the test set\n",
    "test_conf_matrix_efficientnet = confusion_matrix(y_test_true, y_test_pred_efficientnet)\n",
    "test_conf_matrix_resnet = confusion_matrix(y_test_true, y_test_pred_resnet)\n",
    "test_conf_matrix_vgg = confusion_matrix(y_test_true, y_test_pred_vgg)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for EfficientNet:\")\n",
    "print(test_conf_matrix_efficientnet)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for ResNet:\")\n",
    "print(test_conf_matrix_resnet)\n",
    "\n",
    "print(f\"Confusion Matrix (Test) for VGG:\")\n",
    "print(test_conf_matrix_vgg)\n",
    "\n",
    "# Generate the classification report for the test set\n",
    "test_class_report_efficientnet = classification_report(y_test_true, y_test_pred_efficientnet, target_names=categories)\n",
    "test_class_report_resnet = classification_report(y_test_true, y_test_pred_resnet, target_names=categories)\n",
    "test_class_report_vgg = classification_report(y_test_true, y_test_pred_vgg, target_names=categories)\n",
    "\n",
    "print(f\"Classification Report (Test) for EfficientNet:\")\n",
    "print(test_class_report_efficientnet)\n",
    "\n",
    "print(f\"Classification Report (Test) for ResNet:\")\n",
    "print(test_class_report_resnet)\n",
    "\n",
    "print(f\"Classification Report (Test) for VGG:\")\n",
    "print(test_class_report_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54c317-a974-488c-8624-a428a5e08397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis Thanasis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
