{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cfd594-9d32-4e82-a089-016a884f49da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Training model with baseline configuration...\n",
      "Epoch 1/100\n",
      "226/226 [==============================] - 65s 244ms/step - loss: 0.7676 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5153 - lr: 2.4699e-04\n",
      "Epoch 2/100\n",
      "226/226 [==============================] - 54s 238ms/step - loss: 0.7245 - accuracy: 0.5021 - val_loss: 0.6975 - val_accuracy: 0.4205 - lr: 2.4699e-04\n",
      "Epoch 3/100\n",
      "226/226 [==============================] - 54s 239ms/step - loss: 0.7025 - accuracy: 0.5202 - val_loss: 0.6880 - val_accuracy: 0.5484 - lr: 2.4699e-04\n",
      "Epoch 4/100\n",
      "226/226 [==============================] - 54s 238ms/step - loss: 0.6911 - accuracy: 0.5353 - val_loss: 0.6750 - val_accuracy: 0.5861 - lr: 2.4699e-04\n",
      "Epoch 5/100\n",
      "226/226 [==============================] - 54s 239ms/step - loss: 0.6851 - accuracy: 0.5701 - val_loss: 0.6859 - val_accuracy: 0.5882 - lr: 2.4699e-04\n",
      "Epoch 6/100\n",
      "226/226 [==============================] - 54s 239ms/step - loss: 0.6811 - accuracy: 0.5637 - val_loss: 0.8693 - val_accuracy: 0.4205 - lr: 2.4699e-04\n",
      "Epoch 7/100\n",
      "226/226 [==============================] - 54s 240ms/step - loss: 0.6844 - accuracy: 0.5561 - val_loss: 0.6724 - val_accuracy: 0.5948 - lr: 2.4699e-04\n",
      "Epoch 8/100\n",
      "226/226 [==============================] - 54s 240ms/step - loss: 0.6836 - accuracy: 0.5624 - val_loss: 0.6797 - val_accuracy: 0.5791 - lr: 2.4699e-04\n",
      "Epoch 9/100\n",
      "226/226 [==============================] - 54s 238ms/step - loss: 0.6831 - accuracy: 0.5658 - val_loss: 0.6947 - val_accuracy: 0.5439 - lr: 2.4699e-04\n",
      "Epoch 10/100\n",
      "226/226 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.5734\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.9398187547922136e-05.\n",
      "226/226 [==============================] - 54s 239ms/step - loss: 0.6771 - accuracy: 0.5734 - val_loss: 0.6814 - val_accuracy: 0.5824 - lr: 2.4699e-04\n",
      "Epoch 11/100\n",
      "226/226 [==============================] - 54s 238ms/step - loss: 0.6759 - accuracy: 0.5737 - val_loss: 0.6744 - val_accuracy: 0.5770 - lr: 4.9398e-05\n",
      "Epoch 12/100\n",
      "226/226 [==============================] - 54s 238ms/step - loss: 0.6726 - accuracy: 0.5779 - val_loss: 0.6809 - val_accuracy: 0.5666 - lr: 4.9398e-05\n",
      "76/76 [==============================] - 6s 75ms/step\n",
      "Validation Accuracy for baseline: 0.5947847962379456\n",
      "Confusion Matrix (Test) for baseline:\n",
      "[[1468   22]\n",
      " [ 876   50]]\n",
      "Classification Report (Test) for baseline:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.63      0.99      0.77      1490\n",
      "Damaged_augmented       0.69      0.05      0.10       926\n",
      "\n",
      "         accuracy                           0.63      2416\n",
      "        macro avg       0.66      0.52      0.43      2416\n",
      "     weighted avg       0.65      0.63      0.51      2416\n",
      "\n",
      "Training model with no_residual configuration...\n",
      "Epoch 1/100\n",
      " 58/226 [======>.......................] - ETA: 28s - loss: 0.6932 - accuracy: 0.5516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Conv2D, MaxPooling2D, BatchNormalization, Activation, Add, SeparableConv2D, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\Θάνος\\\\Desktop\\\\Thesis Thanasis\\\\data_aug_3'\n",
    "subfolders = ['clear', 'clouds']\n",
    "categories = ['Healthy_augmented', 'Damaged_augmented']\n",
    "IMG_HEIGHT =  32\n",
    "IMG_WIDTH = 32\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # Set to 100\n",
    "\n",
    "# Load Data Function\n",
    "def load_data(base_dir, subfolders, categories, img_height, img_width):\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, subfolder, category)\n",
    "            images = sorted(os.listdir(folder_path))\n",
    "            for img_name in images:\n",
    "                if img_name.endswith('.png'):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                    data.append(img_array)\n",
    "                    labels.append(class_num)\n",
    "                    image_paths.append((subfolder, category, img_name))\n",
    "    return np.array(data), np.array(labels), image_paths\n",
    "\n",
    "# Load the data\n",
    "data, labels, image_paths = load_data(base_dir, subfolders, categories, IMG_HEIGHT, IMG_WIDTH)\n",
    "data = data / 255.0\n",
    "\n",
    "# Split the data\n",
    "def split_data(image_paths):\n",
    "    unique_image_ids = list(set([img_name for subfolder, category, img_name in image_paths]))\n",
    "    train_ids, test_ids = train_test_split(unique_image_ids, test_size=0.2, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "def get_split_indices(image_paths, split_ids):\n",
    "    split_indices = [i for i, (subfolder, category, img_name) in enumerate(image_paths) if img_name in split_ids]\n",
    "    return split_indices\n",
    "\n",
    "train_ids, val_ids, test_ids = split_data(image_paths)\n",
    "train_indices = get_split_indices(image_paths, train_ids)\n",
    "val_indices = get_split_indices(image_paths, val_ids)\n",
    "test_indices = get_split_indices(image_paths, test_ids)\n",
    "\n",
    "# Prepare train, validation, and test sets\n",
    "X_train, y_train = data[train_indices], labels[train_indices]\n",
    "X_val, y_val = data[val_indices], labels[val_indices]\n",
    "X_test, y_test = data[test_indices], labels[test_indices]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Data augmentation\n",
    "def create_datagen(seed=None):\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ), seed\n",
    "\n",
    "datagen, seed = create_datagen(seed=42)\n",
    "datagen.fit(X_train)\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "val_generator = datagen.flow(X_val, y_val, batch_size=BATCH_SIZE, seed=seed)\n",
    "\n",
    "# Compute class weights using the training set\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Best hyperparameters from Optuna (Provided by You)\n",
    "best_model_params = {\n",
    "    'num_residual_blocks': 4, \n",
    "    'filters': 121, \n",
    "    'kernel_size': 5, \n",
    "    'dense_units': 1387, \n",
    "    'dropout_rate': 0.40846071442232423, \n",
    "    'learning_rate': 0.0002469909302048313\n",
    "}\n",
    "\n",
    "# Define ablation configurations\n",
    "ABLATION_CONFIGS = {\n",
    "    \"baseline\": {\"use_residual\": True, \"use_attention\": True, \"use_dropout\": True, \"use_batch_norm\": True},\n",
    "    \"no_residual\": {\"use_residual\": False, \"use_attention\": True, \"use_dropout\": True, \"use_batch_norm\": True},\n",
    "    \"no_attention\": {\"use_residual\": True, \"use_attention\": False, \"use_dropout\": True, \"use_batch_norm\": True},\n",
    "    \"no_dropout\": {\"use_residual\": True, \"use_attention\": True, \"use_dropout\": False, \"use_batch_norm\": True},\n",
    "    \"no_batch_norm\": {\"use_residual\": True, \"use_attention\": True, \"use_dropout\": True, \"use_batch_norm\": False},\n",
    "}\n",
    "\n",
    "# Function to build model with ablation configuration\n",
    "def build_model(config):\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    # First convolutional layer with the best filter size and kernel size\n",
    "    x = Conv2D(best_model_params['filters'], (best_model_params['kernel_size'], best_model_params['kernel_size']), padding='same', activation='relu')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Add residual blocks based on the best hyperparameters\n",
    "    for _ in range(best_model_params['num_residual_blocks']):\n",
    "        if config[\"use_residual\"]:\n",
    "            x = residual_block(x, best_model_params['filters'], best_model_params['kernel_size'], config)\n",
    "        if config[\"use_attention\"]:\n",
    "            x = attention_block(x, best_model_params['filters'])\n",
    "        if x.shape[1] >= 2 and x.shape[2] >= 2:\n",
    "            x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Dropout based on the best dropout rate\n",
    "    if config[\"use_dropout\"]:\n",
    "        x = Dropout(best_model_params['dropout_rate'])(x)\n",
    "    \n",
    "    # Dense layer using the best number of units\n",
    "    x = Dense(best_model_params['dense_units'], activation='relu')(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Compile the model with the best learning rate\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=best_model_params['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Residual block definition\n",
    "def residual_block(x, filters, kernel_size, config):\n",
    "    shortcut = x\n",
    "    x = SeparableConv2D(filters, (kernel_size, kernel_size), padding='same')(x)\n",
    "    if config[\"use_batch_norm\"]:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = SeparableConv2D(filters, (kernel_size, kernel_size), padding='same')(x)\n",
    "    if config[\"use_batch_norm\"]:\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, (1, 1), padding='same')(shortcut)\n",
    "        if config[\"use_batch_norm\"]:\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Attention block definition\n",
    "def attention_block(x, filters):\n",
    "    attention = MultiHeadAttention(num_heads=8, key_dim=filters)(x, x)\n",
    "    attention = Add()([x, attention])\n",
    "    return attention\n",
    "\n",
    "# Function to train and evaluate model for ablation study\n",
    "def train_and_evaluate_model(config_name, config, X_train, y_train, X_val, y_val, class_weights):\n",
    "    print(f\"Training model with {config_name} configuration...\")\n",
    "    model = build_model(config)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    val_accuracy = np.max(history.history['val_accuracy'])\n",
    "    return model, val_accuracy\n",
    "\n",
    "# Perform ablation study\n",
    "def ablation_study(X_train, y_train, X_val, y_val, X_test, y_test, class_weights):\n",
    "    results = {}\n",
    "    for config_name, config in ABLATION_CONFIGS.items():\n",
    "        model, val_accuracy = train_and_evaluate_model(config_name, config, X_train, y_train, X_val, y_val, class_weights)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions = model.predict(X_test)\n",
    "        y_test_true = np.argmax(y_test, axis=1)\n",
    "        y_test_pred = np.argmax(test_predictions, axis=1)\n",
    "        test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "        test_class_report = classification_report(y_test_true, y_test_pred, target_names=categories)\n",
    "        \n",
    "        # Store results\n",
    "        results[config_name] = {\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"confusion_matrix\": test_conf_matrix,\n",
    "            \"classification_report\": test_class_report\n",
    "        }\n",
    "        \n",
    "        print(f\"Validation Accuracy for {config_name}: {val_accuracy}\")\n",
    "        print(f\"Confusion Matrix (Test) for {config_name}:\")\n",
    "        print(test_conf_matrix)\n",
    "        print(f\"Classification Report (Test) for {config_name}:\")\n",
    "        print(test_class_report)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the ablation study\n",
    "ablation_results = ablation_study(X_train, y_train, X_val, y_val, X_test, y_test, class_weights)\n",
    "\n",
    "# Output the results\n",
    "for config_name, result in ablation_results.items():\n",
    "    print(f\"Results for {config_name}:\")\n",
    "    print(f\"Validation Accuracy: {result['val_accuracy']}\")\n",
    "    print(f\"Confusion Matrix:\\n{result['confusion_matrix']}\")\n",
    "    print(f\"Classification Report:\\n{result['classification_report']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa7d50-7997-47ae-9d07-c34e319d8c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eca27b-9c29-40f7-a44a-929a30db35d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e74df3-beb6-4c61-9969-78d4e3d600ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
