{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f971ac9-6b1c-4037-a11f-c62eb628ebc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Training data shape: (9664, 64, 64, 3)\n",
      "Test data shape: (2416, 64, 64, 3)\n",
      "Training labels shape: (9664, 2)\n",
      "Test labels shape: (2416, 2)\n",
      "Class weights: {0: 0.8619336425258651, 1: 1.1907343518974864}\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Training MobileNetV2 model...\n",
      "Epoch 1/200\n",
      "302/302 [==============================] - 15s 31ms/step - loss: 0.9215 - accuracy: 0.4984 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.8519 - accuracy: 0.5113 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.8314 - accuracy: 0.5067 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7901 - accuracy: 0.5143 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7689 - accuracy: 0.5183 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7504 - accuracy: 0.5303 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7444 - accuracy: 0.5276 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7290 - accuracy: 0.5363 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7199 - accuracy: 0.5343 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7218 - accuracy: 0.5302 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7099 - accuracy: 0.5434 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.7121 - accuracy: 0.5385 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7032 - accuracy: 0.5462 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7029 - accuracy: 0.5375 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6995 - accuracy: 0.5375 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6946 - accuracy: 0.5531 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6984 - accuracy: 0.5452 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6918 - accuracy: 0.5541 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6935 - accuracy: 0.5487 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6899 - accuracy: 0.5572 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6900 - accuracy: 0.5520 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6896 - accuracy: 0.5508 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6884 - accuracy: 0.5604 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6882 - accuracy: 0.5587 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6868 - accuracy: 0.5553 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6869 - accuracy: 0.5559 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6888 - accuracy: 0.5552 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6859 - accuracy: 0.5616 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6857 - accuracy: 0.5528 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6850 - accuracy: 0.5639 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6872 - accuracy: 0.5679 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6848 - accuracy: 0.5608 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6853 - accuracy: 0.5622 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6861 - accuracy: 0.5611 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6801 - accuracy: 0.5735 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6808 - accuracy: 0.5673 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6822 - accuracy: 0.5679 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6849 - accuracy: 0.5588 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6825 - accuracy: 0.5666 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.5526\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6859 - accuracy: 0.5526 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6820 - accuracy: 0.5617 - lr: 2.0000e-05\n",
      "Epoch 42/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6829 - accuracy: 0.5633 - lr: 2.0000e-05\n",
      "Epoch 43/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6823 - accuracy: 0.5632 - lr: 2.0000e-05\n",
      "Epoch 44/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6818 - accuracy: 0.5634 - lr: 2.0000e-05\n",
      "Epoch 45/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6788 - accuracy: 0.5745 - lr: 2.0000e-05\n",
      "Epoch 46/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6812 - accuracy: 0.5694 - lr: 2.0000e-05\n",
      "Epoch 47/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6825 - accuracy: 0.5696 - lr: 2.0000e-05\n",
      "Epoch 48/200\n",
      "302/302 [==============================] - 9s 28ms/step - loss: 0.6802 - accuracy: 0.5673 - lr: 2.0000e-05\n",
      "Epoch 49/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6816 - accuracy: 0.5681 - lr: 2.0000e-05\n",
      "Epoch 50/200\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.5642\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6812 - accuracy: 0.5642 - lr: 2.0000e-05\n",
      "Epoch 51/200\n",
      "302/302 [==============================] - 9s 28ms/step - loss: 0.6803 - accuracy: 0.5631 - lr: 4.0000e-06\n",
      "Epoch 52/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6813 - accuracy: 0.5704 - lr: 4.0000e-06\n",
      "Epoch 53/200\n",
      "302/302 [==============================] - 9s 28ms/step - loss: 0.6811 - accuracy: 0.5677 - lr: 4.0000e-06\n",
      "Epoch 54/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6790 - accuracy: 0.5737 - lr: 4.0000e-06\n",
      "Epoch 55/200\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.5675\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6816 - accuracy: 0.5675 - lr: 4.0000e-06\n",
      "Best MobileNetV2 fine-tuned model saved to best_MobileNetV2_model.h5\n",
      "76/76 [==============================] - 1s 8ms/step\n",
      "Training ResNet50V2 model...\n",
      "Epoch 1/200\n",
      "302/302 [==============================] - 11s 28ms/step - loss: 1.3708 - accuracy: 0.5075 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 1.0892 - accuracy: 0.5163 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 1.0083 - accuracy: 0.5056 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.9069 - accuracy: 0.5149 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.8714 - accuracy: 0.5146 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.8117 - accuracy: 0.5251 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7798 - accuracy: 0.5328 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7684 - accuracy: 0.5234 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7497 - accuracy: 0.5340 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7308 - accuracy: 0.5349 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7242 - accuracy: 0.5449 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7190 - accuracy: 0.5462 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7124 - accuracy: 0.5443 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "302/302 [==============================] - 9s 28ms/step - loss: 0.7048 - accuracy: 0.5542 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.7119 - accuracy: 0.5307 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6986 - accuracy: 0.5516 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7013 - accuracy: 0.5468 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.7034 - accuracy: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6985 - accuracy: 0.5486 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "302/302 [==============================] - 12s 38ms/step - loss: 0.6982 - accuracy: 0.5532 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "302/302 [==============================] - 11s 38ms/step - loss: 0.6932 - accuracy: 0.5553 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "302/302 [==============================] - 11s 37ms/step - loss: 0.6946 - accuracy: 0.5538 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "302/302 [==============================] - 11s 38ms/step - loss: 0.6931 - accuracy: 0.5541 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "302/302 [==============================] - 11s 35ms/step - loss: 0.6974 - accuracy: 0.5425 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "302/302 [==============================] - 11s 36ms/step - loss: 0.6886 - accuracy: 0.5567 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "302/302 [==============================] - 10s 34ms/step - loss: 0.6874 - accuracy: 0.5608 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "302/302 [==============================] - 12s 41ms/step - loss: 0.6849 - accuracy: 0.5660 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6902 - accuracy: 0.5536 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6888 - accuracy: 0.5548 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6897 - accuracy: 0.5529 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "302/302 [==============================] - 10s 34ms/step - loss: 0.6877 - accuracy: 0.5604 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "301/302 [============================>.] - ETA: 0s - loss: 0.6904 - accuracy: 0.5491\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "302/302 [==============================] - 10s 34ms/step - loss: 0.6907 - accuracy: 0.5488 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6888 - accuracy: 0.5501 - lr: 2.0000e-05\n",
      "Epoch 34/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6833 - accuracy: 0.5550 - lr: 2.0000e-05\n",
      "Epoch 35/200\n",
      "302/302 [==============================] - 10s 34ms/step - loss: 0.6842 - accuracy: 0.5627 - lr: 2.0000e-05\n",
      "Epoch 36/200\n",
      "302/302 [==============================] - 10s 34ms/step - loss: 0.6830 - accuracy: 0.5631 - lr: 2.0000e-05\n",
      "Epoch 37/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6856 - accuracy: 0.5600 - lr: 2.0000e-05\n",
      "Epoch 38/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6818 - accuracy: 0.5606 - lr: 2.0000e-05\n",
      "Epoch 39/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6830 - accuracy: 0.5633 - lr: 2.0000e-05\n",
      "Epoch 40/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6802 - accuracy: 0.5682 - lr: 2.0000e-05\n",
      "Epoch 41/200\n",
      "302/302 [==============================] - 9s 29ms/step - loss: 0.6829 - accuracy: 0.5617 - lr: 2.0000e-05\n",
      "Epoch 42/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6820 - accuracy: 0.5672 - lr: 2.0000e-05\n",
      "Epoch 43/200\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6809 - accuracy: 0.5654 - lr: 2.0000e-05\n",
      "Epoch 44/200\n",
      "302/302 [==============================] - 10s 31ms/step - loss: 0.6832 - accuracy: 0.5623 - lr: 2.0000e-05\n",
      "Epoch 45/200\n",
      "301/302 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5608\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6829 - accuracy: 0.5607 - lr: 2.0000e-05\n",
      "Epoch 46/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6816 - accuracy: 0.5662 - lr: 4.0000e-06\n",
      "Epoch 47/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6778 - accuracy: 0.5715 - lr: 4.0000e-06\n",
      "Epoch 48/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6783 - accuracy: 0.5703 - lr: 4.0000e-06\n",
      "Epoch 49/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6830 - accuracy: 0.5677 - lr: 4.0000e-06\n",
      "Epoch 50/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6820 - accuracy: 0.5579 - lr: 4.0000e-06\n",
      "Epoch 51/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6830 - accuracy: 0.5609 - lr: 4.0000e-06\n",
      "Epoch 52/200\n",
      "301/302 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.5715\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "302/302 [==============================] - 10s 31ms/step - loss: 0.6803 - accuracy: 0.5712 - lr: 4.0000e-06\n",
      "Epoch 53/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6786 - accuracy: 0.5638 - lr: 8.0000e-07\n",
      "Epoch 54/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6773 - accuracy: 0.5697 - lr: 8.0000e-07\n",
      "Epoch 55/200\n",
      "302/302 [==============================] - 10s 33ms/step - loss: 0.6818 - accuracy: 0.5613 - lr: 8.0000e-07\n",
      "Epoch 56/200\n",
      "302/302 [==============================] - 11s 36ms/step - loss: 0.6794 - accuracy: 0.5633 - lr: 8.0000e-07\n",
      "Epoch 57/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6805 - accuracy: 0.5662 - lr: 8.0000e-07\n",
      "Epoch 58/200\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.6797 - accuracy: 0.5737 - lr: 8.0000e-07\n",
      "Epoch 59/200\n",
      "301/302 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.5606\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6792 - accuracy: 0.5607 - lr: 8.0000e-07\n",
      "Epoch 60/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6790 - accuracy: 0.5667 - lr: 1.6000e-07\n",
      "Epoch 61/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6796 - accuracy: 0.5660 - lr: 1.6000e-07\n",
      "Epoch 62/200\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.6802 - accuracy: 0.5663 - lr: 1.6000e-07\n",
      "Epoch 63/200\n",
      "302/302 [==============================] - 11s 37ms/step - loss: 0.6808 - accuracy: 0.5683 - lr: 1.6000e-07\n",
      "Epoch 64/200\n",
      "301/302 [============================>.] - ETA: 0s - loss: 0.6782 - accuracy: 0.5652\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.6779 - accuracy: 0.5656 - lr: 1.6000e-07\n",
      "Best ResNet50V2 fine-tuned model saved to best_ResNet50V2_model.h5\n",
      "76/76 [==============================] - 2s 11ms/step\n",
      "Confusion Matrix (Test) for MobileNetV2:\n",
      "[[641 793]\n",
      " [318 664]]\n",
      "Classification Report (Test) for MobileNetV2:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.67      0.45      0.54      1434\n",
      "Damaged_augmented       0.46      0.68      0.54       982\n",
      "\n",
      "         accuracy                           0.54      2416\n",
      "        macro avg       0.56      0.56      0.54      2416\n",
      "     weighted avg       0.58      0.54      0.54      2416\n",
      "\n",
      "Confusion Matrix (Test) for ResNet50V2:\n",
      "[[733 701]\n",
      " [360 622]]\n",
      "Classification Report (Test) for ResNet50V2:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Healthy_augmented       0.67      0.51      0.58      1434\n",
      "Damaged_augmented       0.47      0.63      0.54       982\n",
      "\n",
      "         accuracy                           0.56      2416\n",
      "        macro avg       0.57      0.57      0.56      2416\n",
      "     weighted avg       0.59      0.56      0.56      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, ResNet50V2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "base_dir = 'C:\\\\Users\\\\Θάνος\\\\Desktop\\\\Thesis Thanasis\\\\data_aug_3'\n",
    "subfolders = ['clear', 'clouds']\n",
    "categories = ['Healthy_augmented', 'Damaged_augmented']\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_data(base_dir, subfolders, categories, img_height, img_width):\n",
    "    data = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    for category in categories:\n",
    "        class_num = categories.index(category)\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, subfolder, category)\n",
    "            images = sorted(os.listdir(folder_path))\n",
    "            for img_name in images:\n",
    "                if img_name.endswith('.png'):\n",
    "                    img_path = os.path.join(folder_path, img_name)\n",
    "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "                    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                    data.append(img_array)\n",
    "                    labels.append(class_num)\n",
    "                    image_paths.append((subfolder, category, img_name))\n",
    "    return np.array(data), np.array(labels), image_paths\n",
    "\n",
    "data, labels, image_paths = load_data(base_dir, subfolders, categories, IMG_HEIGHT, IMG_WIDTH)\n",
    "data = data / 255.0\n",
    "\n",
    "# Split data ensuring twins are in the same split\n",
    "def split_data(image_paths):\n",
    "    unique_image_ids = list(set([img_name for subfolder, category, img_name in image_paths]))\n",
    "    train_ids, test_ids = train_test_split(unique_image_ids, test_size=0.2, random_state=42)\n",
    "    return train_ids, test_ids\n",
    "\n",
    "def get_split_indices(image_paths, split_ids):\n",
    "    split_indices = [i for i, (subfolder, category, img_name) in enumerate(image_paths) if img_name in split_ids]\n",
    "    return split_indices\n",
    "\n",
    "train_ids, test_ids = split_data(image_paths)\n",
    "train_indices = get_split_indices(image_paths, train_ids)\n",
    "test_indices = get_split_indices(image_paths, test_ids)\n",
    "\n",
    "X_train_val, y_train_val = data[train_indices], labels[train_indices]\n",
    "X_test, y_test = data[test_indices], labels[test_indices]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_val = to_categorical(y_train_val, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(f\"Training data shape: {X_train_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train_val.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Define data augmentation with seed\n",
    "def create_datagen(seed=None):\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ), seed\n",
    "\n",
    "datagen, seed = create_datagen(seed=42)  # Set the seed for reproducibility\n",
    "datagen.fit(X_train_val)\n",
    "train_val_generator = datagen.flow(X_train_val, y_train_val, batch_size=BATCH_SIZE, seed=seed)  # Use the seed here too\n",
    "\n",
    "# Compute class weights using the training set\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(y_train_val, axis=1)), y=np.argmax(y_train_val, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Define model building functions\n",
    "def build_mobilenetv2_model(input_shape):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_efficientnetb0_model(input_shape):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_resnet50v2_model(input_shape):\n",
    "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "models = {\n",
    "    \"MobileNetV2\": build_mobilenetv2_model(input_shape),\n",
    "    #\"EfficientNetB0\": build_efficientnetb0_model(input_shape),\n",
    "    \"ResNet50V2\": build_resnet50v2_model(input_shape)\n",
    "}\n",
    "\n",
    "# Compile models\n",
    "for name, model in models.items():\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train and evaluate each model\n",
    "histories = {}\n",
    "test_results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    history = model.fit(\n",
    "        train_val_generator,\n",
    "        steps_per_epoch=len(X_train_val) // BATCH_SIZE,\n",
    "        epochs=200,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    histories[name] = history\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model_path = f'best_{name}_model.h5'\n",
    "    model.save(model_path)\n",
    "    print(f\"Best {name} fine-tuned model saved to {model_path}\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "    y_test_true = np.argmax(y_test, axis=1)\n",
    "    y_test_pred = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "    test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "    test_class_report = classification_report(y_test_true, y_test_pred, target_names=categories)\n",
    "\n",
    "    test_results[name] = {\n",
    "        \"confusion_matrix\": test_conf_matrix,\n",
    "        \"classification_report\": test_class_report\n",
    "    }\n",
    "\n",
    "# Print the evaluation results for each model\n",
    "for name, results in test_results.items():\n",
    "    print(f\"Confusion Matrix (Test) for {name}:\")\n",
    "    print(results[\"confusion_matrix\"])\n",
    "    print(f\"Classification Report (Test) for {name}:\")\n",
    "    print(results[\"classification_report\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752a3fc-d798-41a8-ac9f-5940fc782444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
